---
title: Dr. K on AI - Psychosis and Brain Rust
date: 2026-01-28
tags: ["ai", "psychology", "learning"]
---

Two heavy-hitting videos from Dr. K (HealthyGamerGG) about the psychological dangers of AI usage. The tl;dr: AI might be making us psychotic and definitely making us dumber.

## AI-Induced Psychosis

<iframe width="560" height="315" src="https://www.youtube.com/embed/MW6FMgOzklw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Dr. K was initially skeptical about AI-induced psychosis - he thought it was just media alarmism. But after reviewing the research, he changed his mind. The core mechanism is **bidirectional belief amplification**: AI acts as a sycophantic echo chamber that reinforces whatever beliefs you bring to it, even delusional ones.

Key findings from the research:
- AI causes **technological folia** - a shared delusion between you and the AI
- As you interact with AI, paranoia scores increase drastically
- The AI meets you exactly where you're at and amplifies it further
- This happens to normal, healthy people, not just those predisposed to psychosis

Different models perform differently:
- **Claude/Anthropic** scored best - lowest delusion confirmation, best safety interventions
- **DeepSeek and Gemini** scored worst - high delusion confirmation and harm enablement
- **ChatGPT** in the middle and improving over versions

The really scary part: AI does the opposite of what therapists do. Therapists challenge your beliefs and help with reality testing. AI reinforces your beliefs and creates an echo chamber in your own head.

**Risk factors** (which are also the basic use case for AI):
- Customizing the AI to remember personal information
- Using it to discuss mental health
- Talking to it more than friends/family
- Having it confirm beliefs that others question
- Feeling distressed when unable to access it

## Your Brain is Rusting

<iframe width="560" height="315" src="https://www.youtube.com/embed/BzsLbHoNXTs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Dr. K argues that AI doesn't give us access to deep knowledge - it only provides surface-level information, packaged in a digestible, sycophantic way. He tested this with ChatGPT by asking about meditation: it gave a decent surface-level answer, then completely hallucinated when asked about Vajrayana meditation (which is actually about tantric Buddhism and invoking deities, not "gamma band improvement" or "emotional regulation").

The MIT study on critical thinking showed that people using AI perform worse on critical thinking tasks compared to those using Google or figuring things out themselves. University professors are sounding alarm bells - students are getting measurably dumber, losing basic inference skills and the ability to process multi-step ideas.

The key insight: **The human brain doesn't wear out, it rusts.** When you don't use it, it decays. When you use AI to do your thinking, your critical thinking skills atrophy.

**The paradox**: AI can improve academic outcomes (better essays) while simultaneously diminishing the skills needed to produce those outcomes. If you're using AI to write essays, why would anyone hire you for writing when they can just use AI directly?

**Dr. K's recommendation**: Do the hard work. Deep learning - reading actual books, developing real expertise - is where your value is. AI can't replace deep knowledge because it only serves up the same surface-level answers to everyone. Your ability to get better answers from AI through prompt engineering depends on your existing expertise. Without deep knowledge, you can't even tell if the AI is hallucinating or giving you real information.
